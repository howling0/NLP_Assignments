{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习上课内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.回答以下理论问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 请写一下TF-IDF的计算公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF计算公式如下：\n",
    "$$TF-IDF(x)=TF(x) \\times IDF(x)$$  \n",
    "\n",
    "其中$TF(x)$为词*x*在文档*d*中出现的频率，即词频，计算公式为：  \n",
    "$$TF(x)=\\frac{count(x \\space occurrence)}{count(Total \\space words \\space in \\space Document)}$$  \n",
    "\n",
    "由于对于文档*d*来说，分母对于每个词都是一样的，所以可以省略分母，并增加log变换使各词的计算结果差值变小，变成以下形式：\n",
    "$$TF(x)=log(count(x \\space occurrence))$$  \n",
    "\n",
    "*TF还有一种常见的形式，分母使用该文档中出现最多的词的出现次数，对应形式为:$TF(x)=\\frac{count(x \\space occurrence)}{count(most \\space frequent \\space word \\space occurrence)}$*\n",
    "\n",
    "\n",
    "$IDF(x)$为词*x*的逆文档频率（Inverse Document Frequency），通过用文档总数除以包含词*x*的文档数目得到，使用加一平滑避免除0的问题，并在结果上增加log变换，其公式为：\n",
    "$$IDF(x)=log(\\frac{count(Documents) + 1}{count(Documents \\space with \\space x \\space occurred) + 1}) + 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. LDA算法的基本假设是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA即Latent Dirichlet Allocation，隐含狄利克雷分布，其基本假设是文档主题以狄利克雷分布为先验分布，而每个主题中词的先验分布也是狄利克雷分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 在TextRank算法中构建图的权重是如何得到的？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextRank首先使用滑窗方式获取词之间的连边，连边权重通过计算两个词的相似性（距离）得出。在图构建完毕后，开始进行迭代，即对每个节点取其相连节点的权重，使用$WS(v_i)':=(1-d) + d \\times \\sum{v_j\\in In(v_i)}\\frac{w_{ij}}{\\sum{v_k}\\in Out(v_j)w_{jk}}WS(v_j)$进行计算，不断迭代更新各个节点的权重，直至收敛。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 什么是命名实体识别？ 有什么应用场景？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "命名实体识别即Named Entity Recognition(NER)，是信息提取的一个子任务，旨在从大段的文本数据中提取出命名实体并进行分类，例如人名、地名、组织、货币、金额、日期等等。  \n",
    "NER的应用范围非常广泛，例如文本摘要，简历关键词提取，通过抽取商品介绍内容进行更精准的分类和推荐，都可以使用NER或与之类似的思想来完成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.NLP主要有哪几类任务 ？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP任务多种多样，常见的NLP任务如：\n",
    " - 语音识别\n",
    " - 机器翻译\n",
    " - 自动问答客服\n",
    " - 文本分类\n",
    " - 自动摘要\n",
    " - 文本/语音生成\n",
    " - 关键词提取\n",
    " - 分词\n",
    " - 知识图谱"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.实践题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 手动实现TextRank算法 (在新闻数据中随机提取100条新闻训练词向量和做做法测试）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 提示：\n",
    " 1. 确定窗口，建立图链接。   \n",
    " 2. 通过词向量相似度确定图上边的权重\n",
    " 3. 根据公式实现算法迭代(d=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from collections import defaultdict\n",
    "from gensim.models import word2vec\n",
    "import re\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"新华社数据.csv\", encoding=\"gb18030\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机提取100条数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data = data.sample(n=100, random_state=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 100 entries, 42742 to 34304\n",
      "Data columns (total 7 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   id       100 non-null    int64 \n",
      " 1   author   86 non-null     object\n",
      " 2   source   100 non-null    object\n",
      " 3   content  100 non-null    object\n",
      " 4   feature  100 non-null    object\n",
      " 5   title    100 non-null    object\n",
      " 6   url      100 non-null    object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 6.2+ KB\n"
     ]
    }
   ],
   "source": [
    "sub_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取停止词\n",
    "with open(\"./stopwords.txt\", encoding=\"utf-8-sig\") as f:\n",
    "    stopwords = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_it(content, stopwords=None):\n",
    "    # 去除标点及特殊字符\n",
    "    content = re.sub('[!；：．·、…【】《》“”‘’！？\"#$%&％\\'?@，。〔〕［］（）()*+,\\\\./:;<=>＋×／'\n",
    "                      '①↑↓★▌▲●℃[\\\\]^_`{|}\\s\\\\\\\\n]+',\n",
    "                      \"\",\n",
    "                      content)\n",
    "    words = jieba.lcut(content)\n",
    "    # 过滤停用词\n",
    "    if stopwords:\n",
    "        filter_words = [word for word in words if word not in stopwords]\n",
    "    else:\n",
    "        filter_words = words\n",
    "    return \" \".join(filter_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'新华社 照片 利马 年月日 \\\\ n 体育 趣味 狗拉松 月 日 一名 男子 带领 宠物犬 参赛 当日 秘鲁 首都 利马 一场 宠物犬 参加 马拉松赛 比赛 中 宠物 主人 带领 宠物犬 千米 道路 奔跑 新华社 发 路易斯 卡马乔 摄'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"新华社照片利马年月日\\\\n体育趣味狗拉松\\n月日一名男子带领宠物犬参赛\\n当日秘鲁首都利马一场宠物犬参加马拉松赛比赛中宠物主人带领宠物犬千米道路奔跑\\n新华社发路易斯卡马乔摄\"\n",
    "cut_it(test, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data['content'] = sub_data['content'].apply(cut_it, stopwords=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = [content.split(\" \") for content in sub_data['content']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model = word2vec.Word2Vec(contents, size=50, window=5, min_count=1, iter=10, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建节点和图类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class node:\n",
    "    '''\n",
    "    节点\n",
    "    '''\n",
    "    def __init__(self, word=None, weight=0):\n",
    "        '''\n",
    "        初始化节点信息\n",
    "        --------------\n",
    "        word: 节点包含的词\n",
    "        weight: 初始权重\n",
    "        '''\n",
    "        self.word = word\n",
    "        self.weight = weight\n",
    "        self.edges = defaultdict(float)\n",
    "        self.out_edge_total = 0\n",
    "    \n",
    "    def __getitem__(self, word2):\n",
    "        '''\n",
    "        获取到word2的连边权重\n",
    "        -------------------\n",
    "        word2: 获取连边权重的目标词\n",
    "        '''\n",
    "        return self.edges[word2]\n",
    "        \n",
    "    def calc_similarity(self, word2, method='cosine'):\n",
    "        '''\n",
    "        计算相似度\n",
    "        -----------\n",
    "        word2: 节点所连的词\n",
    "        method: 计算方式\n",
    "        '''\n",
    "        similarity = 0\n",
    "        if method == 'cosine':\n",
    "            similarity = cosine(wv_model.wv[self.word], wv_model.wv[word2])\n",
    "        elif method == 'L2':\n",
    "            similarity = np.sqrt(np.sum([pow(x, 2) for x in (wv_model.wv[self.word] - wv_model.wv[word2])]))\n",
    "        else:\n",
    "            similarity = np.abs(wv_model.wv[self.word] - wv_model.wv[word2])\n",
    "            if len(similarity) > 1:\n",
    "                similarity = sum(similarity)\n",
    "        return similarity\n",
    "        \n",
    "    def add_edge_to(self, w2, similarity=None, include_co_occur=False):\n",
    "        '''\n",
    "        添加连边\n",
    "        ---------\n",
    "        w2: 需要连接的词或节点\n",
    "        similarity: 词语相似度\n",
    "        '''\n",
    "        if similarity == None:\n",
    "            if not include_co_occur and w2.word in self.edges:\n",
    "                return\n",
    "            # 等于None时进行计算，并为所连词添加连边\n",
    "            similarity = self.calc_similarity(w2.word)\n",
    "            self.edges[w2.word] += similarity\n",
    "            w2.add_edge_to(self.word, similarity, include_co_occur)\n",
    "        else:\n",
    "            if not include_co_occur and w2 in self.edges:\n",
    "                return\n",
    "            # 不等于None时直接记录连边权重\n",
    "            self.edges[w2] += similarity\n",
    "        # 更新连边权重总和\n",
    "        self.out_edge_total += similarity\n",
    "        \n",
    "    def remove_edge_to(self, w2):\n",
    "        '''\n",
    "        移除连边\n",
    "        --------\n",
    "        w2: 需要断开连接的词或节点\n",
    "        '''\n",
    "        if type(w2) == node:\n",
    "            similarity = self.edges.pop(w2.word)\n",
    "            w2.remove_edge_to(self.word)\n",
    "        else:\n",
    "            similarity = self.edges.pop(w2)\n",
    "        # 更新连边权重总和\n",
    "        self.out_edge_total -= similarity\n",
    "        \n",
    "    def is_linked_to(self, w2):\n",
    "        '''\n",
    "        判断是否与目标词相连\n",
    "        --------------------\n",
    "        w2: 目标词\n",
    "        '''\n",
    "        return self.edges[w2] != 0\n",
    "    \n",
    "    def update(self, graph, converged_value=1e-7, d=0.85):\n",
    "        '''\n",
    "        更新节点权重\n",
    "        -------------\n",
    "        graph: 图的引用\n",
    "        converged_value: 判断是否收敛的依据\n",
    "        '''\n",
    "        converged = True\n",
    "        if not self.edges:\n",
    "            return converged\n",
    "        new_weight = 0\n",
    "        for word2, similarity in self.edges.items():\n",
    "            opposite_node = graph[word2]\n",
    "            if opposite_node.word != None:\n",
    "                new_weight += opposite_node.weight * (similarity/opposite_node.out_edge_total)\n",
    "        new_weight = (1 - d) + d * new_weight\n",
    "        if abs(new_weight - self.weight) > 1e-7:\n",
    "            converged = False\n",
    "        self.weight = new_weight\n",
    "        return converged\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class graph:\n",
    "    '''\n",
    "    图\n",
    "    '''\n",
    "    def __init__(self, contents, d=0.85):\n",
    "        '''\n",
    "        初始化图信息\n",
    "        -------\n",
    "        contents: 分词后的句子列表\n",
    "        '''\n",
    "        if not contents:\n",
    "            return\n",
    "        # 初始化\n",
    "        self.d = d\n",
    "        self.nodes = defaultdict(node)\n",
    "        self.contents = contents\n",
    "        seen_words = set()\n",
    "        # 生成节点\n",
    "        for content in contents:\n",
    "            for word in content:\n",
    "                # 跳过已记录的词\n",
    "                if word in seen_words:\n",
    "                    continue\n",
    "                self.nodes[word] = node(word, 0)\n",
    "                seen_words.add(word)\n",
    "                \n",
    "        # 更新节点初始权重\n",
    "        n = len(seen_words)\n",
    "        del seen_words\n",
    "        init_weight = 1/n\n",
    "        for node_ in self.nodes.values():\n",
    "            node_.weight = init_weight\n",
    "            \n",
    "    def __getitem__(self, word):\n",
    "        '''\n",
    "        获取word对应节点\n",
    "        -------------------\n",
    "        word: 目标词\n",
    "        '''\n",
    "        return self.nodes[word]\n",
    "    \n",
    "    def build_edges(self, window_size=5, include_co_occur=False):\n",
    "        '''\n",
    "        使用滑动窗口，为所有节点建立连边\n",
    "        --------------------------------\n",
    "        window_size: 窗口大小\n",
    "        include_co_occur: 是否考虑词语共现次数\n",
    "        '''\n",
    "        if not self.contents:\n",
    "            raise Exception(\"图已建立或没有输入正确的contents内容！\")\n",
    "        n = window_size >> 1\n",
    "        # 遍历句子\n",
    "        for content in self.contents:\n",
    "            # 遍历词语\n",
    "            for i in range(len(content)):\n",
    "                w1 = content[i]\n",
    "                sub_content = []\n",
    "                if i < n:\n",
    "                    sub_content = content[:i] + content[i+1:i+n+1]\n",
    "                elif i + n >= len(content):\n",
    "                    sub_content = content[i-n:i] + content[i+1:]\n",
    "                else:\n",
    "                    sub_content = content[i-n:i] + content[i+1:i+n+1]\n",
    "                for w2 in sub_content:\n",
    "                    self.nodes[w1].add_edge_to(self.nodes[w2], include_co_occur=include_co_occur)\n",
    "        \n",
    "        # 清除\n",
    "        self.contents = None\n",
    "    \n",
    "    def start_iterations(self, max_iter=100, converged_value=1e-7):\n",
    "        '''\n",
    "        开始迭代更新节点参数\n",
    "        --------------------\n",
    "        max_iter: 最大迭代次数\n",
    "        converged_value: 判断是否收敛的依据\n",
    "        '''\n",
    "        # 初始化flag和迭代计数\n",
    "        converged = False\n",
    "        count = 0\n",
    "        # 循环直到收敛\n",
    "        while not converged:\n",
    "            converged = True\n",
    "            # 遍历节点，更新权重，记录权重参数是否变更\n",
    "            for node_ in self.nodes.values():\n",
    "                converged &= node_.update(self, converged_value, self.d)\n",
    "            # 增加计数\n",
    "            count += 1\n",
    "            # 判断达到最大迭代次数，退出\n",
    "            if count >= max_iter:\n",
    "                break\n",
    "        # 返回最终结果是否收敛\n",
    "        return converged, count\n",
    "    \n",
    "    def most_important(self, text, n=5):\n",
    "        '''\n",
    "        获取文本中权重最高的n个词\n",
    "        -------------------------\n",
    "        text: 分词后的文本\n",
    "        n: 返回数目\n",
    "        '''\n",
    "        if type(text) == str:\n",
    "            text = text.split(\" \")\n",
    "        result = {(word, self[word].weight) for word in text}\n",
    "        result = sorted(result, key=lambda x: x[1], reverse=True)\n",
    "        return result[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankMultiText(contents):\n",
    "    G = graph(contents)\n",
    "    G.build_edges()\n",
    "    print(G.start_iterations())\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankSingleText(content):\n",
    "    G = graph([content])\n",
    "    G.build_edges()\n",
    "    print(G.start_iterations())\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(G, text, n=20):\n",
    "    print(\"\".join(text))\n",
    "    return G.most_important(text, n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, 60)\n"
     ]
    }
   ],
   "source": [
    "G = rankMultiText(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('雷摄', 1.2433893978595734),\n",
       " ('漯河', 1.2090365439653397),\n",
       " ('品酒', 1.1985060721635818),\n",
       " ('拉斯', 1.1939423978328705),\n",
       " ('福州', 1.19217748939991),\n",
       " ('勒斯', 1.1817264407873154),\n",
       " ('柏林', 1.17827570438385),\n",
       " ('青岛', 1.1651315689086914),\n",
       " ('冲凉', 1.1604227125644684),\n",
       " ('爱不释手', 1.1482814699411392),\n",
       " ('奔跑', 1.144282504916191),\n",
       " ('对阵', 1.1440399289131165),\n",
       " ('快讯', 1.140726163983345),\n",
       " ('南昌', 1.1388383507728577),\n",
       " ('利兹联', 1.1336669921875),\n",
       " ('大本钟', 1.1304892748594284),\n",
       " ('赫尔辛基', 1.124107114970684),\n",
       " ('太原', 1.1234295144677162),\n",
       " ('庞兴雷', 1.1233981475234032),\n",
       " ('平摄', 1.1229473128914833)]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(G[\"新华社\"].edges.items()), key=lambda x:x[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "新华社照片利马2017年4月24日体育2趣味狗拉松4月23日一名男子带领宠物犬参赛当日秘鲁首都利马一场宠物犬参加马拉松赛比赛中宠物主人带领宠物犬15千米道路奔跑新华社发路易斯卡马乔摄\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('新华社', 26.658834014239588),\n",
       " ('年', 20.239226631534653),\n",
       " ('日', 18.702198049253877),\n",
       " ('月', 16.202038665004682),\n",
       " ('中', 16.024042785365843),\n",
       " ('4', 9.495607263515184),\n",
       " ('2017', 7.885232289863415),\n",
       " ('体育', 5.670393356770726),\n",
       " ('当日', 5.396655531806085),\n",
       " ('照片', 4.914455612849356),\n",
       " ('比赛', 4.848614888694893),\n",
       " ('摄', 4.711916351525653),\n",
       " ('2', 3.494097564568777),\n",
       " ('带领', 3.4539619680179467),\n",
       " ('参加', 2.6060575592722754),\n",
       " ('首都', 2.4794486862646785),\n",
       " ('发', 2.3233665665303644),\n",
       " ('一名', 2.2352885975541334),\n",
       " ('15', 1.9199825904869372),\n",
       " ('道路', 1.8478514323789126)]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank(G, contents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一以深化放管服改革进一步铲除滋生腐败土壤减权限权是预防腐败釜底抽薪之策政府一场深刻革命政府部门壮士断腕勇气扎实举措推进放管服改革企业群众广开便利门政府履职廉洁高效清单管理推动减权规范用权放管服改革减权限权数量成效实践中发现领域审批事项保留审批事项名义一项实际上细分小项成权力套娃部门自行设置类事项减了名目增加特别审批事项相关不合时宜精简空间很大审批抬高制度性交易成本寻租腐败简政放权改革持续深化限度权力压减到位规范铲除滋生腐败土壤实行清单管理制度深化放管服改革抓手建立规范政府权力责任总台账所有权责事项详尽列入清单明明白白社会公布清单之内政府部门履职尽责清单之外禁止擅自设权扩权切实权力置于制度框架规范运行抓紧制定国务院部门权力责任清单清单制定减权过程减则减除涉及公共利益事项外行政审批事项原则上依法程序取消市场准入负面清单试点加快制定工商登记前置审批事项清单企业设立经营许可清单确需保留实行多证合一证照联办证照分离试点推动实体经济转型升级涉及工业产品审批许可地方同志工业生产领域审批产品涉及十大类地方化工产品配方制造业工艺流程审批配方工艺企业商业秘密审批做企业耗费时间贻误市场先机加大知识产权外泄风险影响企业创新积极性导致寻租清理大幅压减工业产品生产许可证确需保留制定清单严格管理企业创新清障松绑政府性基金行政事业性收费中介机构行业协会商会涉企收费实行清单管理各类清单公开晾晒社会监督腐败藏身地二要创新事中事后监管保障廉洁执法放权减权政府部门精力转事中事后监管两年推行双随机公开监管随机抽查加执法公开监管对象头上利剑高悬始终感到监管无形压力心存侥幸任性检查和执法过程中人为干扰减少监管部门寻租腐败双随机公开监管全覆盖加快推进跨部门联合检查有利于减轻企业负担检查部门相互监督信用监管联合惩戒建国家企业信用信息归集系统推动部门间地区间涉企信息交换共享提高监管效能推进综合执法改革规范公正文明执法重复执法多头执法粗暴执法减少企业干扰智能监管数据监管监管全过程留痕权力滥用三要优化服务政务清廉推行互联网政务服务加快国务院部门地方政府信息系统互联互通全国统一政务服务平台国务院部门信息平台互不联通措施打通信息孤岛提高企业群众办事便利性效率监督减少办事人员吃拿卡要机会便民高效廉洁政风进一步提升实体政务大厅服务能力水平加快网上服务平台融合发展网上办上网实行一号申请一窗受理一网通办减证便民专项证明手续取消取消合并合并程度利企便民努力营造实施创新驱动发展战略促双创增就业加快新旧动能转换良好环境切实管好公共资金公帑靡费一笔管好围绕分配运行三个环节管理监督确保资金绩效一是预算分配规范透明管好资金抓住预算龙头增强预算编制完整性科学性透明度政府收支纳入预算预算盘子收入笼子支出口子加快建立健全科学规范透明专项资金分配管理办法严控新设专项转移支付项目加快专项资金清理整合大幅压缩小散低效专项资金资金碎片化加大财政支出优化整合力度重点清理常年安排用途固化支出加大减税降费力度降低企业负担政府紧日子中央部门要带头一律低于5幅度压减一般性支出决不允许增加三公经费这件事说到做到地方政府压减着力打造阳光财政财政资金部门公开预决算从今年起地方财政部门网上设立预决算统一公开平台专栏政府预决算部门预决算网上公开群众找看得懂监督二是资金拨付到位转移支付资金预算项目安排衔接畅途时间长到位迟延影响项目进展民生保障还会年底突击花钱引发廉政风险对此提前做好项目论证审批前期工作确保预算批复拨付钱项目各类转移支付预算批复提高预拨比例加快推进国库收付制度改革推行电子化支付方式减少审核环节财政资金拨付到位见效三是资金高效几年一系列措施资金沉淀抓抓降一降阵子反弹国库库款居高不下解决地方扶贫民生资金长期滞留闲置财政收入增速放缓民生支出需求增加情况决不能资金趴在账上睡大觉加大力度清理盘活沉淀财政资金收回存量资金加快安排二次沉淀进一步提高专项资金统筹效益切实解决酱油钱买醋特别支持贫困县统筹整合财政涉农资金试点严肃财经纪律严厉查处小金库截留挪用骗取套取贪污侵占财政资金违法违规行为着力整治公款私存吃利息用于投资理财谋私利违规行为公共资金装锁防盗门进一步国资国企金融监管国有资产财富金融国民经济血脉措施增强国有企业活力金融业稳健发展国有资产流失防范金融领域腐败风险严防国有企业重组改制中徇私舞弊国有企业公司制改制推进混合所有制改革过程中大胆改革创新国有企业瘦身健体提质增效损公肥私侵吞国有资产腐败规范执行三重一大决策制度充分发挥党组织重组改制中把关定向作用日常管理整合监督力量监督合力资本监督重组改制操作流程重点环节要从严监管重点部门关键岗位廉洁风险要从严防控贱卖国资违规交易利益输送违纪违法行为决不允许国有资产装进私人口袋未完待续\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('中', 16.024042785365843),\n",
       " ('发展', 14.972746995946505),\n",
       " ('企业', 9.716790674036147),\n",
       " ('政府', 8.553104338116862),\n",
       " ('5', 8.54035031621638),\n",
       " ('国家', 8.333450671861629),\n",
       " ('群众', 7.714644556577309),\n",
       " ('新', 6.976208682396747),\n",
       " ('创新', 6.6478824799576905),\n",
       " ('资金', 6.393837908616948),\n",
       " ('经济', 6.072268637466087),\n",
       " ('市场', 5.3640820618644796),\n",
       " ('推动', 5.234606313645496),\n",
       " ('项目', 4.823531190120483),\n",
       " ('时间', 4.8049392339402415),\n",
       " ('相关', 4.785162524177267),\n",
       " ('平台', 4.592819936006553),\n",
       " ('生产', 4.526150706413196),\n",
       " ('管理', 4.312600823954643),\n",
       " ('部门', 4.233017202802898)]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank(G, contents[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "新华社照片巴黎2017年6月7日体育2网球——法网巴辛斯基晋级半决赛6月6日巴辛斯基比赛中回球当日法国巴黎2017法国网球公开赛女子单打四分之一决赛中瑞士选手巴辛斯基以20战胜法国选手梅拉德诺维奇晋级半决赛新华社记者陈益宸摄\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('新华社', 26.658834014239588),\n",
       " ('年', 20.239226631534653),\n",
       " ('日', 18.702198049253877),\n",
       " ('记者', 17.811202525006497),\n",
       " ('月', 16.202038665004682),\n",
       " ('中', 16.024042785365843),\n",
       " ('—', 9.750064989967399),\n",
       " ('2017', 7.885232289863415),\n",
       " ('体育', 5.670393356770726),\n",
       " ('当日', 5.396655531806085),\n",
       " ('选手', 5.10530770656837),\n",
       " ('照片', 4.914455612849356),\n",
       " ('比赛', 4.848614888694893),\n",
       " ('摄', 4.711916351525653),\n",
       " ('7', 3.983290104986436),\n",
       " ('战胜', 3.8151792213713622),\n",
       " ('2', 3.494097564568777),\n",
       " ('6', 3.044742106964373),\n",
       " ('法国', 3.02216743601831),\n",
       " ('晋级', 2.710344721637468)]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank(G, contents[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, 52)\n"
     ]
    }
   ],
   "source": [
    "G = rankSingleText(contents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('奔跑', 1.144282504916191),\n",
       " ('利马', 1.0305679179728031),\n",
       " ('路易斯', 0.869931772351265),\n",
       " ('道路', 0.7804607003927231),\n",
       " ('照片', 0.7421922981739044),\n",
       " ('发', 0.5765703320503235)]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(G[\"新华社\"].edges.items()), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "新华社照片利马2017年4月24日体育2趣味狗拉松4月23日一名男子带领宠物犬参赛当日秘鲁首都利马一场宠物犬参加马拉松赛比赛中宠物主人带领宠物犬15千米道路奔跑新华社发路易斯卡马乔摄\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('宠物犬', 2.466519434019129),\n",
       " ('利马', 2.122733764813277),\n",
       " ('带领', 1.4200025883446132),\n",
       " ('新华社', 1.3772103176738515),\n",
       " ('趣味', 1.198522744880457),\n",
       " ('狗拉松', 1.191482100905572),\n",
       " ('路易斯', 1.176636989593093),\n",
       " ('4', 1.1086256599420334),\n",
       " ('奔跑', 1.0935885022231164),\n",
       " ('马拉松赛', 1.078783708872828),\n",
       " ('发', 1.0294562352635714),\n",
       " ('男子', 1.0260332846697315),\n",
       " ('秘鲁', 1.022790098457936),\n",
       " ('千米', 0.999306787977641),\n",
       " ('参赛', 0.9806810896000895),\n",
       " ('日', 0.971867107197245),\n",
       " ('宠物', 0.9705771996222836),\n",
       " ('主人', 0.970456309474833),\n",
       " ('首都', 0.9273991291362361),\n",
       " ('一名', 0.9267133081468271)]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank(G, contents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, 55)\n",
      "一以深化放管服改革进一步铲除滋生腐败土壤减权限权是预防腐败釜底抽薪之策政府一场深刻革命政府部门壮士断腕勇气扎实举措推进放管服改革企业群众广开便利门政府履职廉洁高效清单管理推动减权规范用权放管服改革减权限权数量成效实践中发现领域审批事项保留审批事项名义一项实际上细分小项成权力套娃部门自行设置类事项减了名目增加特别审批事项相关不合时宜精简空间很大审批抬高制度性交易成本寻租腐败简政放权改革持续深化限度权力压减到位规范铲除滋生腐败土壤实行清单管理制度深化放管服改革抓手建立规范政府权力责任总台账所有权责事项详尽列入清单明明白白社会公布清单之内政府部门履职尽责清单之外禁止擅自设权扩权切实权力置于制度框架规范运行抓紧制定国务院部门权力责任清单清单制定减权过程减则减除涉及公共利益事项外行政审批事项原则上依法程序取消市场准入负面清单试点加快制定工商登记前置审批事项清单企业设立经营许可清单确需保留实行多证合一证照联办证照分离试点推动实体经济转型升级涉及工业产品审批许可地方同志工业生产领域审批产品涉及十大类地方化工产品配方制造业工艺流程审批配方工艺企业商业秘密审批做企业耗费时间贻误市场先机加大知识产权外泄风险影响企业创新积极性导致寻租清理大幅压减工业产品生产许可证确需保留制定清单严格管理企业创新清障松绑政府性基金行政事业性收费中介机构行业协会商会涉企收费实行清单管理各类清单公开晾晒社会监督腐败藏身地二要创新事中事后监管保障廉洁执法放权减权政府部门精力转事中事后监管两年推行双随机公开监管随机抽查加执法公开监管对象头上利剑高悬始终感到监管无形压力心存侥幸任性检查和执法过程中人为干扰减少监管部门寻租腐败双随机公开监管全覆盖加快推进跨部门联合检查有利于减轻企业负担检查部门相互监督信用监管联合惩戒建国家企业信用信息归集系统推动部门间地区间涉企信息交换共享提高监管效能推进综合执法改革规范公正文明执法重复执法多头执法粗暴执法减少企业干扰智能监管数据监管监管全过程留痕权力滥用三要优化服务政务清廉推行互联网政务服务加快国务院部门地方政府信息系统互联互通全国统一政务服务平台国务院部门信息平台互不联通措施打通信息孤岛提高企业群众办事便利性效率监督减少办事人员吃拿卡要机会便民高效廉洁政风进一步提升实体政务大厅服务能力水平加快网上服务平台融合发展网上办上网实行一号申请一窗受理一网通办减证便民专项证明手续取消取消合并合并程度利企便民努力营造实施创新驱动发展战略促双创增就业加快新旧动能转换良好环境切实管好公共资金公帑靡费一笔管好围绕分配运行三个环节管理监督确保资金绩效一是预算分配规范透明管好资金抓住预算龙头增强预算编制完整性科学性透明度政府收支纳入预算预算盘子收入笼子支出口子加快建立健全科学规范透明专项资金分配管理办法严控新设专项转移支付项目加快专项资金清理整合大幅压缩小散低效专项资金资金碎片化加大财政支出优化整合力度重点清理常年安排用途固化支出加大减税降费力度降低企业负担政府紧日子中央部门要带头一律低于5幅度压减一般性支出决不允许增加三公经费这件事说到做到地方政府压减着力打造阳光财政财政资金部门公开预决算从今年起地方财政部门网上设立预决算统一公开平台专栏政府预决算部门预决算网上公开群众找看得懂监督二是资金拨付到位转移支付资金预算项目安排衔接畅途时间长到位迟延影响项目进展民生保障还会年底突击花钱引发廉政风险对此提前做好项目论证审批前期工作确保预算批复拨付钱项目各类转移支付预算批复提高预拨比例加快推进国库收付制度改革推行电子化支付方式减少审核环节财政资金拨付到位见效三是资金高效几年一系列措施资金沉淀抓抓降一降阵子反弹国库库款居高不下解决地方扶贫民生资金长期滞留闲置财政收入增速放缓民生支出需求增加情况决不能资金趴在账上睡大觉加大力度清理盘活沉淀财政资金收回存量资金加快安排二次沉淀进一步提高专项资金统筹效益切实解决酱油钱买醋特别支持贫困县统筹整合财政涉农资金试点严肃财经纪律严厉查处小金库截留挪用骗取套取贪污侵占财政资金违法违规行为着力整治公款私存吃利息用于投资理财谋私利违规行为公共资金装锁防盗门进一步国资国企金融监管国有资产财富金融国民经济血脉措施增强国有企业活力金融业稳健发展国有资产流失防范金融领域腐败风险严防国有企业重组改制中徇私舞弊国有企业公司制改制推进混合所有制改革过程中大胆改革创新国有企业瘦身健体提质增效损公肥私侵吞国有资产腐败规范执行三重一大决策制度充分发挥党组织重组改制中把关定向作用日常管理整合监督力量监督合力资本监督重组改制操作流程重点环节要从严监管重点部门关键岗位廉洁风险要从严防控贱卖国资违规交易利益输送违纪违法行为决不允许国有资产装进私人口袋未完待续\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('资金', 5.362874586432887),\n",
       " ('清单', 4.508112555449612),\n",
       " ('监管', 4.397294839292606),\n",
       " ('政府', 4.2525421431204045),\n",
       " ('审批', 4.068895802098536),\n",
       " ('加快', 3.860872336356502),\n",
       " ('部门', 3.4014858016634237),\n",
       " ('企业', 3.335006802819762),\n",
       " ('事项', 3.2940794885755844),\n",
       " ('规范', 3.202527453859185),\n",
       " ('改革', 3.1781412389559476),\n",
       " ('腐败', 3.1675287961604885),\n",
       " ('地方', 3.078274429583967),\n",
       " ('监督', 3.0286738819120984),\n",
       " ('权力', 2.88833329271991),\n",
       " ('国有资产', 2.7356686358884983),\n",
       " ('预算', 2.506709025118639),\n",
       " ('实行', 2.408143598505686),\n",
       " ('风险', 2.394682516475521),\n",
       " ('整合', 2.380964805397552)]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = rankSingleText(contents[2])\n",
    "rank(G, contents[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感觉TextRank的效果一般，结果有些类似TF，出现频率越高的词语，最后的权重就越大，在多篇文章同时处理时更为明显，基本每篇文章最重要的词都是新华社…如果不去除停用词和标点的话，排在前面的就全是标点符号和“的”之类的词了…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 选做 1.  提取新闻人物里的对话。(使用以上提取小数据即可）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示：    \n",
    "1.寻找预料里具有表示说的意思。    \n",
    "2.使用语法分析提取句子结构。    \n",
    "3.检测谓语是否有表示说的意思。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 选择2. ： 电影评论分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个作业中你要完成一个电影评论分类任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.数据获取。（采用爬虫技术爬取相关网页上的电影评论数据，例如猫眼电影评论，豆瓣电影评论）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.把所获得数据分解为训练集，验证集和测试集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.选用相应算法构建模型，并测试。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 选择3：文章自动续写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个作业中你要完成一个文章自动续写的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.数据获取。（根据你的兴趣采用爬虫技术爬去相关网站上的文本数据内容：比如故事网站，小说网站等）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.选取模型，并训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.展示一些你模型的输出例子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path, sentence_end=\"\\n\", encoding=\"utf-8\"):\n",
    "    with open(path, \"r\", encoding=encoding) as f:\n",
    "        content = f.read().split(sentence_end)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(texts):\n",
    "    '''\n",
    "    分词\n",
    "    -----------\n",
    "    texts: 句子列表\n",
    "    '''\n",
    "    return [jieba.lcut(text.replace(u'\\u3000',u'')) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tokenizer(texts):\n",
    "    '''\n",
    "    初始化tokenizer\n",
    "    ------------------\n",
    "    texts: 已分词的句子列表\n",
    "    '''\n",
    "    # 添加UNK标识\n",
    "    texts_ = texts + [[\"<UNK>\",\"<NONE>\"]]\n",
    "    # 实例化\n",
    "    tokenizer = Tokenizer(filters='#$%&+-/<=>@[\\\\]^`{|}~\\t\\n')\n",
    "    # 生成倒排表\n",
    "    tokenizer.fit_on_texts(texts_)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(texts, tokenizer):\n",
    "    '''\n",
    "    将句子转化为token列表\n",
    "    ----------------------\n",
    "    texts: 已分词的句子列表\n",
    "    tokenizer: token转换器\n",
    "    '''\n",
    "    # 未知词语转换为UNK标识\n",
    "    texts = [[word if word in tokenizer.word_index else \"<UNK>\" for word in text] for text in texts]\n",
    "    # 将词语转为token\n",
    "    sequence = tokenizer.texts_to_sequences(texts)\n",
    "    # padding\n",
    "    padded = pad_sequences(sequence, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, batch_size, hidden_units):\n",
    "        '''\n",
    "        初始化模型\n",
    "        ---------------------\n",
    "        vocab_size: 词库大小\n",
    "        embedding_dim: 词嵌入维度\n",
    "        batch_size: 批次大小\n",
    "        hidden_units: GRU_1,GRU_2,和FC_1三层的隐层神经元数目列表\n",
    "        '''\n",
    "        assert len(hidden_units) == 3\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_units = hidden_units\n",
    "        # 嵌入层\n",
    "        self.embedding = layers.Embedding(vocab_size, embedding_dim)\n",
    "        # GRU_1\n",
    "        self.gru_1 = layers.GRU(hidden_units[0], return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "        # GRU_2\n",
    "        self.gru_2 = layers.GRU(hidden_units[1], return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "        # FC_1\n",
    "        self.dense_1 = layers.Dense(hidden_units[2])\n",
    "        # FC_2\n",
    "        self.dense_2 = layers.Dense(vocab_size)\n",
    "    \n",
    "    def call(self, x, hidden):\n",
    "        '''\n",
    "        调用模型\n",
    "        -------------------------\n",
    "        x: 数据,(batch_size, sequence_length)\n",
    "        hidden: GRU_1和GRU_2的隐层状态列表\n",
    "        '''\n",
    "        if len(x.shape) <= 1:\n",
    "            x = tf.reshape(x, (-1,1))\n",
    "        # embedding => (batch_size, sequence_length, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        # gru => output(batch_size, sequence_length, hidden_units[0]), state(batch_size, hidden_units[0])\n",
    "        output, state = self.gru_1(x, hidden[0])\n",
    "        # gru_2 => output(batch_size, sequence_length, hidden_units[1]), state(batch_size, hidden_units[1])\n",
    "        output, state_2 = self.gru_2(output, hidden[1])\n",
    "        # reshape => (batch_size*sequence_length, hidden_units[1])\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        # fc_1 => (batch_size, hidden_units[2])\n",
    "        output = self.dense_1(output)\n",
    "        # fc_2 => (batch_size, vocab_size)\n",
    "        output = self.dense_2(output)\n",
    "        # 返回结果\n",
    "        return output, state, state_2\n",
    "    \n",
    "    def initialize_hidden_state(self, batch_size = None):\n",
    "        '''\n",
    "        使用全零向量初始化GRU隐层状态\n",
    "        '''\n",
    "        if not batch_size:\n",
    "            batch_size = self.batch_size\n",
    "        return [tf.zeros((batch_size, self.hidden_units[0])), tf.zeros((batch_size, self.hidden_units[1]))]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoWriter:\n",
    "    def __init__(self, padded_texts):\n",
    "        '''\n",
    "        初始化，建立数据集\n",
    "        -------------------\n",
    "        padded_texts: 分词，转为token并padding后的文本列表\n",
    "        '''\n",
    "        self.dataset = tf.data.Dataset.from_tensor_slices(padded_texts)\n",
    "        self.data_length = len(padded_texts)\n",
    "        \n",
    "    def init_model(self, vocab_size, embedding_dim, batch_size, hidden_units, tokenizer):\n",
    "        '''\n",
    "        初始化模型\n",
    "        ------------------\n",
    "        vocab_size: 词库大小\n",
    "        embedding_dim: 词嵌入维度\n",
    "        batch_size: 批次大小\n",
    "        hidden_units: GRU_1,GRU_2,和FC_1三层的隐层神经元数目列表\n",
    "        tokenizer: token转换器，包含词语-索引的倒排表\n",
    "        '''\n",
    "        self.language_model = LanguageModel(vocab_size, embedding_dim, batch_size, hidden_units)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.set_data_batch(batch_size)\n",
    "        \n",
    "    def load_weights(self, path, token_name=None, include_tokenizer=False):\n",
    "        '''\n",
    "        读取预训练的模型权重\n",
    "        --------------------\n",
    "        path: 权重路径\n",
    "        token_name: token转换器的路径\n",
    "        include_tokenizer: 是否需要同时读取token转换器\n",
    "        '''\n",
    "        self.language_model.load_weights(path)\n",
    "        if include_tokenizer:\n",
    "            if not token_name:\n",
    "                token_name = \"tokenizer.pickle\"\n",
    "            with open(token_name, 'rb') as f:\n",
    "                self.tokenizer = pickle.load(f)\n",
    "        print(\"Model weights loaded\")\n",
    "        \n",
    "    def save_model(self, path, token_name=None, include_tokenizer=True):\n",
    "        '''\n",
    "        保存训练好的模型权重\n",
    "        -----------------\n",
    "        path: 保存模型权重的路径\n",
    "        token_name: token转换器的路径\n",
    "        include_tokenizer: 是否需要同时保存token转换器\n",
    "        '''\n",
    "        self.language_model.save_weights(path)\n",
    "        if include_tokenizer:\n",
    "            if not token_name:\n",
    "                token_name = \"tokenizer.pickle\"\n",
    "            with open(token_name, 'wb') as f:\n",
    "                pickle.dump(self.tokenizer, f)\n",
    "        print(\"Model weights saved to:\", path)\n",
    "        \n",
    "    def set_data_batch(self, batch_size):\n",
    "        '''\n",
    "        设置batch size\n",
    "        ---------------\n",
    "        batch_size: 批次大小\n",
    "        '''\n",
    "        self.steps_per_epoch = self.data_length // batch_size\n",
    "        self.dataset = self.dataset.batch(batch_size, drop_remainder=True)\n",
    "    \n",
    "    def calc_loss(self, label, pred):\n",
    "        '''\n",
    "        计算损失\n",
    "        ----------\n",
    "        label: 标签\n",
    "        pred: 预测结果\n",
    "        '''\n",
    "        # 使用损失函数，获取loss\n",
    "        loss = self.loss_function(label, pred)\n",
    "        # 对label为0的部分生成mask，屏蔽padding的部分\n",
    "        mask = tf.math.logical_not(tf.math.equal(label, 0))\n",
    "        mask = tf.cast(mask, dtype=loss.dtype)\n",
    "        loss *= mask\n",
    "        \n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    def train_step(self, inputs, hidden=None):\n",
    "        '''\n",
    "        单步训练\n",
    "        ----------\n",
    "        inputs: 一个批次的输入内容，(batch_size, sequence_length)\n",
    "        hidden: GRU_1和GRU_2的隐层状态\n",
    "        '''\n",
    "        # 初始化隐层状态\n",
    "        if not hidden:\n",
    "            hidden = self.language_model.initialize_hidden_state()\n",
    "        # 初始化loss\n",
    "        loss = 0\n",
    "        # 设置tape\n",
    "        with tf.GradientTape() as tape:\n",
    "            # 遍历所有时间步\n",
    "            for t in range(1, inputs.shape[1]):\n",
    "                # 调用模型获取预测输出和更新后的GRU隐层状态\n",
    "                pred, hidden[0], hidden[1] = self.language_model(inputs[:, t-1], hidden)\n",
    "                # 调用损失计算，累加损失\n",
    "                loss += self.calc_loss(inputs[:, t], pred)\n",
    "        # 用loss除以sequence_length(time steps)求均值\n",
    "        batch_loss = loss / int(inputs.shape[1])\n",
    "        # 取出模型可训练参数\n",
    "        trainables = self.language_model.trainable_variables\n",
    "        # 计算梯度\n",
    "        gradients = tape.gradient(loss, trainables)\n",
    "        # 更新梯度\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainables))\n",
    "        return batch_loss\n",
    "        \n",
    "    def train(self, loss_function=None, epochs=10, learning_rate=1e-3):\n",
    "        '''\n",
    "        训练模型\n",
    "        -----------------\n",
    "        loss_function: 损失函数\n",
    "        epochs: 训练轮数\n",
    "        learning_rate: 学习率\n",
    "        '''\n",
    "        # 设置损失函数\n",
    "        if not loss_function:\n",
    "            loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "        self.loss_function = loss_function\n",
    "        # 设置优化器\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "        for epoch in range(epochs):\n",
    "            # 记录开始时间\n",
    "            start = time.time()\n",
    "            loss = 0\n",
    "            # 分batch取出数据\n",
    "            for batch, inputs in enumerate(self.dataset.take(self.steps_per_epoch)):\n",
    "                # 训练一步\n",
    "                batch_loss = self.train_step(inputs)\n",
    "                # 累计loss\n",
    "                loss += batch_loss\n",
    "                # 每100个batch打印输出\n",
    "                if batch % 100 == 0:\n",
    "                    print(\"Epoch {} Batch {} Loss {:.4f}\".format(epoch + 1, batch, batch_loss.numpy()))\n",
    "            # 打印epoch信息，并调用eval_检查模型输出\n",
    "            print(\"Epoch {} finished, Loss: {:.4f}, Time cost: {} seconds\\n\".format(epoch+1, loss / self.steps_per_epoch, time.time()-start))\n",
    "            print(self.eval_())\n",
    "            \n",
    "    def eval_(self, sentences=2):\n",
    "        '''\n",
    "        检查模型输出，便于评估\n",
    "        -----------------------\n",
    "        sentences: 输出句子数目\n",
    "        '''\n",
    "        index = int(np.random.random() * (self.steps_per_epoch - 1)) + 1\n",
    "        input_ = list(self.dataset.take(index))[-1][0]\n",
    "        return self.write(input_, sentences=sentences)\n",
    "    \n",
    "    def write(self, pre_text, sentence_end=\"。\", sentences=5, max_words=50):\n",
    "        '''\n",
    "        续写文章\n",
    "        ---------\n",
    "        pre_text: 前文内容\n",
    "        sentence_end: 句子结束标识\n",
    "        sentences: 需要生成的句子数目\n",
    "        max_words: 每个句子最大词数\n",
    "        '''\n",
    "        # 处理不存在于数据中的句子结束标识\n",
    "        if sentence_end not in self.tokenizer.word_index:\n",
    "            raise ValueError(\"{} is not a known word!\".format(sentence_end))\n",
    "        \n",
    "        # 是否需要将输入内容转换为词语\n",
    "        need_mapping = True\n",
    "        # 初始化输出结果\n",
    "        output = \"\"\n",
    "        # 如果输入前文是文本，对其进行分词并转为token，同时记录前文内容\n",
    "        if type(pre_text) == str:\n",
    "            output = pre_text\n",
    "            need_mapping = False\n",
    "            pre_text = cut(pre_text)\n",
    "            pre_text = tokenize(pre_text, self.tokenizer)\n",
    "            \n",
    "        # 转为tensor\n",
    "        pre_text = tf.constant(pre_text)\n",
    "        # 初始化隐层状态\n",
    "        hidden = self.language_model.initialize_hidden_state(1)\n",
    "        # 依次输入前文\n",
    "        for word in pre_text:\n",
    "            # 将index转换为词语记录\n",
    "            if need_mapping:\n",
    "                try:\n",
    "                    output += self.tokenizer.index_word[word.numpy()]\n",
    "                except:\n",
    "                    output += \"\"\n",
    "            # 调用模型\n",
    "            next_, hidden[0], hidden[1] = self.language_model(word, hidden)\n",
    "        # 句子计数\n",
    "        sentence_count = 0\n",
    "        # 单句词语计数\n",
    "        word_count = 0\n",
    "        # 获取模型最后的输出index\n",
    "        next_word_index = np.argmax(next_)\n",
    "        output += \">>>new>>>\"\n",
    "        # 循环直到输出句子数目达到要求\n",
    "        while sentence_count < sentences:\n",
    "            # 调用模型\n",
    "            next_, hidden[0], hidden[1] = self.language_model(next_word_index, hidden)\n",
    "            # 获取输出index\n",
    "            next_word_index = np.argmax(next_)\n",
    "            # 获取对应词语并记录\n",
    "            word = self.tokenizer.index_word[next_word_index]\n",
    "            output += word\n",
    "            word_count += 1\n",
    "            # 如果输出了句末标识，则增加句子计数，重置单句词语计数\n",
    "            if word == sentence_end:\n",
    "                word_count = 0\n",
    "                sentence_count += 1\n",
    "            else:\n",
    "                # 否则增加单句词语计数，并判断是否达到上限\n",
    "                # 达到上限则强制添加句末标识，结束句子，并将标识作为下一次模型调用的输入\n",
    "                word_count += 1\n",
    "                if word_count == max_words:\n",
    "                    output += \"。\"\n",
    "                    word_count = 0\n",
    "                    sentence_count += 1\n",
    "                    next_ = self.tokenizer.word_index[sentence_end]\n",
    "                    \n",
    "        return output    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = read_file(\"./text.txt\",encoding=\"gb18030\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15494"
      ]
     },
     "execution_count": 582,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\u3000\\u3000在黑暗的空间艰难跋涉的我眼前一道仿佛天堂的门凭空开启。'"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = cut(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['在',\n",
       " '黑暗',\n",
       " '的',\n",
       " '空间',\n",
       " '艰难',\n",
       " '跋涉',\n",
       " '的',\n",
       " '我',\n",
       " '眼前',\n",
       " '一道',\n",
       " '仿佛',\n",
       " '天堂',\n",
       " '的',\n",
       " '门',\n",
       " '凭空',\n",
       " '开启',\n",
       " '。']"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = init_tokenizer(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = tokenize(content, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  12,  507,    2,  699, 2859, 3478,    2,    4,  274,  611,  349,\n",
       "       1683,    2, 1810, 1460, 1388,    3,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24530"
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "batch_size = 16\n",
    "hidden_units = [128, 64, 32] # gru_1, gru_2, fc_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = AutoWriter(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.init_model(len(tokenizer.word_index), embedding_dim, batch_size, hidden_units, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights loaded\n"
     ]
    }
   ],
   "source": [
    "writer.load_weights(\"language_model\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'>>>new>>>之极之极化解获益匪浅抗下打了个打了个一箭抗下精嘛2289惊天手伸篮球法相场内外2一分钱很久没很久没列游行中探游行找点。拣到2289崩溃排除不象话气极败坏玩吧玩吧2票人场内外一哄而散逸哥打紧时机未到没人能援军战战兢兢战战兢兢我地动静动静求援失忆失忆。'"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writer.write(content[3][:50], \"。\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.4697\n",
      "Epoch 1 Batch 100 Loss 1.3704\n",
      "Epoch 1 Batch 200 Loss 1.2771\n",
      "Epoch 1 Batch 300 Loss 1.1158\n",
      "Epoch 1 Batch 400 Loss 1.3315\n",
      "Epoch 1 Batch 500 Loss 1.9137\n",
      "Epoch 1 Batch 600 Loss 1.2137\n",
      "Epoch 1 Batch 700 Loss 0.9326\n",
      "Epoch 1 Batch 800 Loss 1.5752\n",
      "Epoch 1 Batch 900 Loss 0.8377\n",
      "Epoch 1 finished, Loss: 1.3130, Time cost: 6205.0008001327515 seconds\n",
      "\n",
      "妈的，是可忍!孰不可忍，刚到现场我就有了想挂人的冲动，忍到现在无非是怕先动手让对方知道名字引起不必要的麻烦。而楚时月现身之后又立刻隐起来不出来制止，没有脸皮是第一，有心想看看我的本事应该是第二。>>>new>>>，，，，，，，，，，，，，，，，，，，，，，，，，。，，，，，，，，，，，，，，，，，，，，，，，，，。\n",
      "Epoch 2 Batch 0 Loss 0.3312\n",
      "Epoch 2 Batch 100 Loss 1.3056\n",
      "Epoch 2 Batch 200 Loss 1.2103\n",
      "Epoch 2 Batch 300 Loss 1.0633\n",
      "Epoch 2 Batch 400 Loss 1.0640\n",
      "Epoch 2 Batch 500 Loss 1.5334\n",
      "Epoch 2 Batch 600 Loss 1.0919\n",
      "Epoch 2 Batch 700 Loss 0.8556\n",
      "Epoch 2 Batch 800 Loss 1.4729\n",
      "Epoch 2 Batch 900 Loss 0.7768\n",
      "Epoch 2 finished, Loss: 1.1714, Time cost: 6386.233181238174 seconds\n",
      "\n",
      "搂着怀里的叶子，我们慢慢前进着，眼前除了那些高低不一却都需要几人合抱的树桩之外，没有任何建筑，也没有任何花花草草。如同梅花桩一般的迷宫……>>>new>>>我，我是是的的的的的的的的的的时候，我的的的的的的的。的的时候，我的的的的的的的的时候，我的的的的的的的的时候。\n",
      "Epoch 3 Batch 0 Loss 0.2794\n",
      "Epoch 3 Batch 100 Loss 1.2293\n",
      "Epoch 3 Batch 200 Loss 1.1094\n",
      "Epoch 3 Batch 300 Loss 1.0088\n",
      "Epoch 3 Batch 400 Loss 0.7963\n",
      "Epoch 3 Batch 500 Loss 1.3086\n",
      "Epoch 3 Batch 600 Loss 0.9902\n",
      "Epoch 3 Batch 700 Loss 0.8111\n",
      "Epoch 3 Batch 800 Loss 1.3837\n",
      "Epoch 3 Batch 900 Loss 0.7190\n",
      "Epoch 3 finished, Loss: 1.0523, Time cost: 20822.657466173172 seconds\n",
      "\n",
      "<div style=\"display:none\">www.cmfu.com发布</div>“嗯，幽灵龙已经被我和叶子送回了死亡国度。而且，这个法杖也是无意中接到的任务。”看着卡莎略微有些生气，我赶紧解释着，表示收尾工作已经顺利完成。>>>new>>>“你，我是知道，我是知道，我是知道，我是知道，我是知道，我是。知道，我是知道，我是知道，我是知道，我是知道，我是知道，我是知道。\n",
      "Epoch 4 Batch 0 Loss 0.2531\n",
      "Epoch 4 Batch 100 Loss 1.1504\n",
      "Epoch 4 Batch 200 Loss 1.0229\n",
      "Epoch 4 Batch 300 Loss 0.9426\n",
      "Epoch 4 Batch 400 Loss 0.6956\n",
      "Epoch 4 Batch 500 Loss 1.2128\n",
      "Epoch 4 Batch 600 Loss 0.9296\n",
      "Epoch 4 Batch 700 Loss 0.7697\n",
      "Epoch 4 Batch 800 Loss 1.3029\n",
      "Epoch 4 Batch 900 Loss 0.6670\n",
      "Epoch 4 finished, Loss: 0.9749, Time cost: 6209.613264083862 seconds\n",
      "\n",
      "随着满面春风的胖子出场，全场再次寂静，谁都不知道这胖子这次会给大家带来什么惊喜。>>>new>>>而我的时候，我也是一个一个<unk>的。而我的，我也是一个一个<unk>的。\n",
      "Epoch 5 Batch 0 Loss 0.2356\n",
      "Epoch 5 Batch 100 Loss 1.0833\n",
      "Epoch 5 Batch 200 Loss 0.9592\n",
      "Epoch 5 Batch 300 Loss 0.8961\n",
      "Epoch 5 Batch 400 Loss 0.6564\n",
      "Epoch 5 Batch 500 Loss 1.1560\n",
      "Epoch 5 Batch 600 Loss 0.8853\n",
      "Epoch 5 Batch 700 Loss 0.7389\n",
      "Epoch 5 Batch 800 Loss 1.2567\n",
      "Epoch 5 Batch 900 Loss 0.6300\n",
      "Epoch 5 finished, Loss: 0.9249, Time cost: 6719.6352455616 seconds\n",
      "\n",
      "更新时间2006-2-15 14:32:00  字数：2567>>>new>>>而我，我也是一个工作室的。而我，我也是一个工作室的。\n"
     ]
    }
   ],
   "source": [
    "writer.train(epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved to: language_model\n"
     ]
    }
   ],
   "source": [
    "writer.save_model(\"language_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.2206\n",
      "Epoch 1 Batch 100 Loss 1.0573\n",
      "Epoch 1 Batch 200 Loss 0.9474\n",
      "Epoch 1 Batch 300 Loss 0.9002\n",
      "Epoch 1 Batch 400 Loss 0.6595\n",
      "Epoch 1 Batch 500 Loss 1.1540\n",
      "Epoch 1 Batch 600 Loss 0.8748\n",
      "Epoch 1 Batch 700 Loss 0.7384\n",
      "Epoch 1 Batch 800 Loss 1.2632\n",
      "Epoch 1 Batch 900 Loss 0.6303\n",
      "Epoch 1 finished, Loss: 0.9197, Time cost: 6814.959317445755 seconds\n",
      "\n",
      "组了两个战士之后跟着他们出了泰西城，本来还想组一个术士的，无奈这年头术士相当稀少，只好作罢，好在开设了货币兑换之后药剂的价格还没有上涨，靠喝药抗显然没有多大问题。>>>new>>>而且，我也是一个<unk>的。而我，我也是一个一个<unk>的。\n",
      "Epoch 2 Batch 0 Loss 0.2083\n",
      "Epoch 2 Batch 100 Loss 1.0063\n",
      "Epoch 2 Batch 200 Loss 0.8835\n",
      "Epoch 2 Batch 300 Loss 0.8517\n",
      "Epoch 2 Batch 400 Loss 0.6185\n",
      "Epoch 2 Batch 500 Loss 1.1103\n",
      "Epoch 2 Batch 600 Loss 0.8386\n",
      "Epoch 2 Batch 700 Loss 0.7163\n",
      "Epoch 2 Batch 800 Loss 1.2132\n",
      "Epoch 2 Batch 900 Loss 0.5939\n",
      "Epoch 2 finished, Loss: 0.8773, Time cost: 6738.136963844299 seconds\n",
      "\n",
      "“……请您实现契约的诺言，赐予我无穷的力量……”>>>new>>>而我，我也是一个一个一个一个一个工作室的时候，我也是一个一个的。而我，我也是一个一个的时候，我也是一个一个的。\n",
      "Epoch 3 Batch 0 Loss 0.2041\n",
      "Epoch 3 Batch 100 Loss 0.9678\n",
      "Epoch 3 Batch 200 Loss 0.8429\n",
      "Epoch 3 Batch 300 Loss 0.8144\n",
      "Epoch 3 Batch 400 Loss 0.5944\n",
      "Epoch 3 Batch 500 Loss 1.0812\n",
      "Epoch 3 Batch 600 Loss 0.8160\n",
      "Epoch 3 Batch 700 Loss 0.6987\n",
      "Epoch 3 Batch 800 Loss 1.1794\n",
      "Epoch 3 Batch 900 Loss 0.5664\n",
      "Epoch 3 finished, Loss: 0.8484, Time cost: 6599.712593078613 seconds\n",
      "\n",
      "如同上次地火之精的守护兽一般，地狱镇守者一声长鸣，巨大的肉翅腾的伸出！直直的朝空中的我扑来！>>>new>>>而且，我也是一个工作室的时候，我也是一个工作室的时候，也是一个工作室的。而且，我也是一个工作室的时候，也是在工作室的工作室，也是在工作室的工作室。\n",
      "Epoch 4 Batch 0 Loss 0.1995\n",
      "Epoch 4 Batch 100 Loss 0.9309\n",
      "Epoch 4 Batch 200 Loss 0.8058\n",
      "Epoch 4 Batch 300 Loss 0.7814\n",
      "Epoch 4 Batch 400 Loss 0.5753\n",
      "Epoch 4 Batch 500 Loss 1.0559\n",
      "Epoch 4 Batch 600 Loss 0.7958\n",
      "Epoch 4 Batch 700 Loss 0.6804\n",
      "Epoch 4 Batch 800 Loss 1.1525\n",
      "Epoch 4 Batch 900 Loss 0.5425\n",
      "Epoch 4 finished, Loss: 0.8238, Time cost: 6844.2397429943085 seconds\n",
      "\n",
      "“呵呵，这个确实是误会，刚才我问了下，其实事情起因也很简单，主要当事人也被朋友你挂了一次，我看，今天这事就算卖我个面子，就这样算了？”楚时月依然一副相当高的姿态，对我和颜悦色的道。>>>new>>>时候，我也是一个的时候，我也是一个的时候，我也是一个的时候，我也。是在工作室的工作室，但，也是一个的时候，我也是在工作室的工作室，但却是。\n",
      "Epoch 5 Batch 0 Loss 0.1950\n",
      "Epoch 5 Batch 100 Loss 0.9001\n",
      "Epoch 5 Batch 200 Loss 0.7726\n",
      "Epoch 5 Batch 300 Loss 0.7525\n",
      "Epoch 5 Batch 400 Loss 0.5599\n",
      "Epoch 5 Batch 500 Loss 1.0343\n",
      "Epoch 5 Batch 600 Loss 0.7777\n",
      "Epoch 5 Batch 700 Loss 0.6628\n",
      "Epoch 5 Batch 800 Loss 1.1272\n",
      "Epoch 5 Batch 900 Loss 0.5211\n",
      "Epoch 5 finished, Loss: 0.8022, Time cost: 6870.13790512085 seconds\n",
      "\n",
      "<div style=\"display:none\">www.cmfu.com发布</div>胖子说着，似乎有些自豪。端起茶杯缓缓说着，“就在纠缠良久仍然没有效果之后，他决定下去升会级——当时所有圣战的玩家都在欢呼着要去海莉兰大找第九职业慰劳慰劳。只有他到处瞎逛。”>>>new>>>时候，我也是一个的时候，我也是一个的时候，我也是在工作室的工作室，但。，我也是一个的时候，我也是在工作室的工作室，但，也是在工作室的工作室，。\n"
     ]
    }
   ],
   "source": [
    "writer.train(epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = read_file(\"./short_text.txt\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = cut(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_2 = init_tokenizer(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = tokenize(content, tokenizer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "batch_size = 1\n",
    "hidden_units = [128, 64, 32] # gru_1, gru_2, fc_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = AutoWriter(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.init_model(len(tokenizer_2.word_index), embedding_dim, batch_size, hidden_units, tokenizer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'旧时好多有传承的人家，吃饭有个规矩：一桌子琳琅佳肴前，先吃三口白饭。长辈一代代教诲：第一口必须先吃饭，而绝不能没吃饭就夹菜。>>>new>>>压个压个压个面食压个面食面食上一餐饭象征性总养生养生本自本自自这个多本自本自本自本自前前供养。占据占据<unk>传承教诲白米饭<unk>琳琅琳琅菜恬淡<unk>陪衬陪衬创伤创伤几口吃饭吃饭。'"
      ]
     },
     "execution_count": 670,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writer.write(content[3][:50], \"。\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.8652\n",
      "Epoch 1 finished, Loss: 2.8190, Time cost: 7.799777984619141 seconds\n",
      "\n",
      "它滋养人身，也颐养人心。 >>>new>>>，，，，，，，，，，，，，，，，，，，，，，，，，。，，，，，，，，，，，，，，，，，，，，，，，，，。\n",
      "Epoch 2 Batch 0 Loss 0.8603\n",
      "Epoch 2 finished, Loss: 2.8023, Time cost: 9.619075536727905 seconds\n",
      "\n",
      "这个规矩有来历。明朝的一部养生专著《遵生八笺》中说到，一位僧人，吃饭总是先淡吃三口：“第一，以知饭之正味。人食多以五味杂之，未有知正味者，若淡食，则本自甘美，初不假外味也。第二，思衣食之从来。第三，思农夫之艰苦。”>>>new>>>，，，，，，，，，，，，，，，，，，，，，，，，，。，，，，，，，，，，，，，，，，，，，，，，，，，。\n",
      "Epoch 3 Batch 0 Loss 0.8521\n",
      "Epoch 3 finished, Loss: 2.7380, Time cost: 9.447187185287476 seconds\n",
      "\n",
      "一餐饭中，饭与菜，何为主，何为次?>>>new>>>，，，，，，，，，，，，，，，，，，，，，，，，，。，，，，，，，，，，，，，，，，，，，，，，，，，。\n",
      "Epoch 4 Batch 0 Loss 0.8311\n",
      "Epoch 4 finished, Loss: 2.6794, Time cost: 8.892837285995483 seconds\n",
      "\n",
      "饭，的确是最养人的东西。一方水土养一方人，大江南北饭不同：南方人吃白米饭，类似于北方人吃馒头面条，也类似于西北高原吃特色面食……一餐一饭，化成人的血肉，供养着人的生命。漂泊时，它抚平你心头的创伤;安逸时，它像你初恋时分的纯洁思想。>>>new>>>，，，，，，，，，，，，，，，，，，，，，，，，，。，，，，，，，，，，，，，，，，，，，，，，，，，。\n",
      "Epoch 5 Batch 0 Loss 0.8257\n",
      "Epoch 5 finished, Loss: 2.6080, Time cost: 8.411806106567383 seconds\n",
      "\n",
      "在宴席上，饭，一般被称为主食;然而，它总在宴席的尾声，才姗姗上来。那时，美酒肴馔已占据胃肠;饭，往往成了陪衬。大家，也总象征性地来几口，压个轴。>>>new>>>，，，，，，，，，，，，，，，，，，，，，，，，，。，，，，，，，，，，，，，，，，，，，，，，，，，。\n",
      "Epoch 6 Batch 0 Loss 0.8233\n",
      "Epoch 6 finished, Loss: 2.5704, Time cost: 10.753643274307251 seconds\n",
      "\n",
      "旧时好多有传承的人家，吃饭有个规矩：一桌子琳琅佳肴前，先吃三口白饭。长辈一代代教诲：第一口必须先吃饭，而绝不能没吃饭就夹菜。>>>new>>>，，，，，，，，，，，，，，，，，，，，，，，，，。，，，，，，，，，，，，，，，，，，，，，，，，，。\n",
      "Epoch 7 Batch 0 Loss 0.8177\n",
      "Epoch 7 finished, Loss: 2.5471, Time cost: 8.299912929534912 seconds\n",
      "\n",
      "饭，的确是最养人的东西。一方水土养一方人，大江南北饭不同：南方人吃白米饭，类似于北方人吃馒头面条，也类似于西北高原吃特色面食……一餐一饭，化成人的血肉，供养着人的生命。漂泊时，它抚平你心头的创伤;安逸时，它像你初恋时分的纯洁思想。>>>new>>>，，，，，，，，，，，，，，，，，，，，，，，，，。，，，，，，，，，，，，，，，，，，，，，，，，，。\n",
      "Epoch 8 Batch 0 Loss 0.8111\n",
      "Epoch 8 finished, Loss: 2.5278, Time cost: 9.108729124069214 seconds\n",
      "\n",
      "旧时好多有传承的人家，吃饭有个规矩：一桌子琳琅佳肴前，先吃三口白饭。长辈一代代教诲：第一口必须先吃饭，而绝不能没吃饭就夹菜。>>>new>>>，，，，，，，，，，，，，，，，，，，，，，，，，。，，，，，，，，，，，，，，，，，，，，，，，，，。\n",
      "Epoch 9 Batch 0 Loss 0.8052\n",
      "Epoch 9 finished, Loss: 2.5126, Time cost: 9.939938068389893 seconds\n",
      "\n",
      "在宴席上，饭，一般被称为主食;然而，它总在宴席的尾声，才姗姗上来。那时，美酒肴馔已占据胃肠;饭，往往成了陪衬。大家，也总象征性地来几口，压个轴。>>>new>>>，，，，，，，，，，，，，，，，，，，，，，，，，。，，，，，，，，，，，，，，，，，，，，，，，，，。\n",
      "Epoch 10 Batch 0 Loss 0.8004\n",
      "Epoch 10 finished, Loss: 2.5000, Time cost: 8.990443229675293 seconds\n",
      "\n",
      "一餐饭中，饭与菜，何为主，何为次?>>>new>>>，，，，，，，，，，，，，，，，，，，，，，，，，。，，，，，，，，，，，，，，，，，，，，，，，，，。\n",
      "Epoch 11 Batch 0 Loss 0.7947\n",
      "Epoch 11 finished, Loss: 2.4876, Time cost: 8.355564594268799 seconds\n",
      "\n",
      "我想，三口白饭，是提醒你：食之本，在饭;饭之味，为源。饭味为正味，正味恬淡素朴。一碗白饭的味道，是百味之基。饭之甘，更在百味之上。其甘，是符合自然之道的味，是粮食本身的甘，其美，是得自日月山川的美。>>>new>>>，，，，，，，，，，，，，，，，，，，，，，，，，。，，，，，，，，，，，，，，，，，，，，，，，，，。\n",
      "Epoch 12 Batch 0 Loss 0.7880\n",
      "Epoch 12 finished, Loss: 2.4741, Time cost: 8.463093519210815 seconds\n",
      "\n",
      "旧时好多有传承的人家，吃饭有个规矩：一桌子琳琅佳肴前，先吃三口白饭。长辈一代代教诲：第一口必须先吃饭，而绝不能没吃饭就夹菜。>>>new>>>，，，，，，，，，，，，，，，，，，，，，，，，，。，，，，，，，，，，，，，，，，，，，，，，，，，。\n",
      "Epoch 13 Batch 0 Loss 0.7816\n",
      "Epoch 13 finished, Loss: 2.4590, Time cost: 8.35899019241333 seconds\n",
      "\n",
      "我想，三口白饭，是提醒你：食之本，在饭;饭之味，为源。饭味为正味，正味恬淡素朴。一碗白饭的味道，是百味之基。饭之甘，更在百味之上。其甘，是符合自然之道的味，是粮食本身的甘，其美，是得自日月山川的美。>>>new>>>，，，，，，，，，，，，，，，，，，，，，，，，，。，，，，，，，，，，，，，，，，，，，，，，，，，。\n",
      "Epoch 14 Batch 0 Loss 0.7720\n",
      "Epoch 14 finished, Loss: 2.4392, Time cost: 8.353487253189087 seconds\n",
      "\n",
      "一餐饭中，饭与菜，何为主，何为次?>>>new>>>，，，，，，，，，，，，，，，，，，，，，，，，，。，，，，，，，，，，，，，，，，，，，，，，，，，。\n",
      "Epoch 15 Batch 0 Loss 0.7606\n",
      "Epoch 15 finished, Loss: 2.4130, Time cost: 8.84156084060669 seconds\n",
      "\n",
      "旧时好多有传承的人家，吃饭有个规矩：一桌子琳琅佳肴前，先吃三口白饭。长辈一代代教诲：第一口必须先吃饭，而绝不能没吃饭就夹菜。>>>new>>>，，，，，，，，，，，，，，三口，，，，，，，，，，。，，，，三口，，，，，，，，，，，，，，三口，，，，，。\n",
      "Epoch 16 Batch 0 Loss 0.7461\n",
      "Epoch 16 finished, Loss: 2.3838, Time cost: 9.834321975708008 seconds\n",
      "\n",
      "它滋养人身，也颐养人心。 >>>new>>>，，，，，，，，，，，，，，，，，，，，，，三口，，。，，，，，，，三口，，，，，，，，，三口，，，，，，，。\n",
      "Epoch 17 Batch 0 Loss 0.7279\n",
      "Epoch 17 finished, Loss: 2.3508, Time cost: 8.420578956604004 seconds\n",
      "\n",
      "一餐饭中，饭与菜，何为主，何为次?>>>new>>>，，，上来三口，，，，三口，，，三口，，，，三口，，，三口，，。，，三口，，，三口，，，，三口，，，三口，，，，三口，，，三口。\n",
      "Epoch 18 Batch 0 Loss 0.7070\n",
      "Epoch 18 finished, Loss: 2.2933, Time cost: 8.781811237335205 seconds\n",
      "\n",
      "旧时好多有传承的人家，吃饭有个规矩：一桌子琳琅佳肴前，先吃三口白饭。长辈一代代教诲：第一口必须先吃饭，而绝不能没吃饭就夹菜。>>>new>>>，，：，：，，：，，吃，，三口，，，，三口，，，三口，，。，，三口，，，，三口，，，，三口，，，，三口，，，，三口，，。\n",
      "Epoch 19 Batch 0 Loss 0.6941\n",
      "Epoch 19 finished, Loss: 2.2349, Time cost: 8.3749361038208 seconds\n",
      "\n",
      "旧时好多有传承的人家，吃饭有个规矩：一桌子琳琅佳肴前，先吃三口白饭。长辈一代代教诲：第一口必须先吃饭，而绝不能没吃饭就夹菜。>>>new>>>：：：：：：：：：必须。，：：必须，：：：：：：：：：必须。\n",
      "Epoch 20 Batch 0 Loss 0.6757\n",
      "Epoch 20 finished, Loss: 2.1891, Time cost: 8.48178482055664 seconds\n",
      "\n",
      "饭，的确是最养人的东西。一方水土养一方人，大江南北饭不同：南方人吃白米饭，类似于北方人吃馒头面条，也类似于西北高原吃特色面食……一餐一饭，化成人的血肉，供养着人的生命。漂泊时，它抚平你心头的创伤;安逸时，它像你初恋时分的纯洁思想。>>>new>>>，美酒必须，，：：。，：：：。\n",
      "Epoch 21 Batch 0 Loss 0.6546\n",
      "Epoch 21 finished, Loss: 2.1174, Time cost: 8.439976453781128 seconds\n",
      "\n",
      "它滋养人身，也颐养人心。 >>>new>>>：必须的，：：：：：：：：：：：：：：：：：：：：：。：：：：：：：：：：：：：：：：：：：：：：：：：。\n",
      "Epoch 22 Batch 0 Loss 0.6377\n",
      "Epoch 22 finished, Loss: 2.0539, Time cost: 8.363579034805298 seconds\n",
      "\n",
      "在宴席上，饭，一般被称为主食;然而，它总在宴席的尾声，才姗姗上来。那时，美酒肴馔已占据胃肠;饭，往往成了陪衬。大家，也总象征性地来几口，压个轴。>>>new>>>：：：：：：：：：：：：：：：：：：：：：：：：：。：：：：：：：：：：：：：：：：：：：：：：：：：。\n",
      "Epoch 23 Batch 0 Loss 0.6088\n",
      "Epoch 23 finished, Loss: 1.9716, Time cost: 10.059000015258789 seconds\n",
      "\n",
      "它滋养人身，也颐养人心。 >>>new>>>是高原高原高原高原高原高原高原滋养滋养滋养中，，才姗姗的的，三口，美酒在在饭，。饭，它绝不能是高原高原高原必须必须的的，三口，才姗姗的，三口，绝不能：：：在。\n",
      "Epoch 24 Batch 0 Loss 0.6018\n",
      "Epoch 24 finished, Loss: 1.8972, Time cost: 8.43532419204712 seconds\n",
      "\n",
      "这个规矩有来历。明朝的一部养生专著《遵生八笺》中说到，一位僧人，吃饭总是先淡吃三口：“第一，以知饭之正味。人食多以五味杂之，未有知正味者，若淡食，则本自甘美，初不假外味也。第二，思衣食之从来。第三，思农夫之艰苦。”>>>new>>>多未有才姗姗，三口，绝不能绝不能吃吃：：：：：：：：：：：：：：：。：：：：：：：：：：：：：：：：：：：：：：：：：。\n",
      "Epoch 25 Batch 0 Loss 0.5563\n",
      "Epoch 25 finished, Loss: 1.8137, Time cost: 8.44921088218689 seconds\n",
      "\n",
      "一餐饭中，饭与菜，何为主，何为次?>>>new>>>淡食，也饭，也吃饭吃吃，淡食，饭，它绝不能：：：：：：：：：。：：：：：：：：：：：：：：：：：：：：：：：：：。\n",
      "Epoch 26 Batch 0 Loss 0.5344\n",
      "Epoch 26 finished, Loss: 1.7264, Time cost: 8.379564046859741 seconds\n",
      "\n",
      "一餐饭中，饭与菜，何为主，何为次?>>>new>>>百味在饭，也吃饭吃吃吃，淡食，先吃，吃饭吃吃吃吃，：：：：。：：：：：：：：：：：：：：：：：：：：：：：：：。\n",
      "Epoch 27 Batch 0 Loss 0.5011\n",
      "Epoch 27 finished, Loss: 1.6310, Time cost: 8.387705326080322 seconds\n",
      "\n",
      "一餐饭中，饭与菜，何为主，何为次?>>>new>>>百味在饭，也饭，先吃吃，淡食，也：：：：：：：：：：：。：：：：：：：：：：：：：：：：：：：：：：：：：。\n",
      "Epoch 28 Batch 0 Loss 0.4746\n",
      "Epoch 28 finished, Loss: 1.5530, Time cost: 8.340267658233643 seconds\n",
      "\n",
      "这个规矩有来历。明朝的一部养生专著《遵生八笺》中说到，一位僧人，吃饭总是先淡吃三口：“第一，以知饭之正味。人食多以五味杂之，未有知正味者，若淡食，则本自甘美，初不假外味也。第二，思衣食之从来。第三，思农夫之艰苦。”>>>new>>>自然的它面食滋养饭，饭，它?的高原有的总是三口，饭三口，：：：：。：：：：：：：：：：：：：：：：：：：：：：：：：。\n",
      "Epoch 29 Batch 0 Loss 0.4452\n",
      "Epoch 29 finished, Loss: 1.4728, Time cost: 8.47090220451355 seconds\n",
      "\n",
      "它滋养人身，也颐养人心。 >>>new>>>你你的吃饭吃吃，淡食，饭，它绝不能绝不能：：：：：：：：：：：。：：：：：：：：：：：：：：：：：：：：：：：：：。\n",
      "Epoch 30 Batch 0 Loss 0.4279\n",
      "Epoch 30 finished, Loss: 1.4011, Time cost: 9.967296123504639 seconds\n",
      "\n",
      "在宴席上，饭，一般被称为主食;然而，它总在宴席的尾声，才姗姗上来。那时，美酒肴馔已占据胃肠;饭，往往成了陪衬。大家，也总象征性地来几口，压个轴。>>>new>>>百味在饭，它称为在在饭，它抚平你的抚平的有的的三口，饭，?吃。吃三口，饭：：：：：：：：：：：：：：：：：：：：：。\n",
      "Epoch 31 Batch 0 Loss 0.3985\n",
      "Epoch 31 finished, Loss: 1.3203, Time cost: 8.48712944984436 seconds\n",
      "\n",
      "我想，三口白饭，是提醒你：食之本，在饭;饭之味，为源。饭味为正味，正味恬淡素朴。一碗白饭的味道，是百味之基。饭之甘，更在百味之上。其甘，是符合自然之道的味，是粮食本身的甘，其美，是得自日月山川的美。>>>new>>>符合自然的它你抚平的它你抚平的它你的有的的三口，饭，?吃吃：。：：：：：：：：：：：：：：：：：：：：：：：：：。\n",
      "Epoch 32 Batch 0 Loss 0.3831\n",
      "Epoch 32 finished, Loss: 1.2445, Time cost: 8.478025674819946 seconds\n",
      "\n",
      "一餐饭中，饭与菜，何为主，何为次?>>>new>>>你你的它绝不能吃：：：：：：：：：：：：：：：：：：：。：：：：：：：：：：：：：：：：：：：：：：：：：。\n",
      "Epoch 33 Batch 0 Loss 0.3605\n",
      "Epoch 33 finished, Loss: 1.1788, Time cost: 8.42824912071228 seconds\n",
      "\n",
      "我想，三口白饭，是提醒你：食之本，在饭;饭之味，为源。饭味为正味，正味恬淡素朴。一碗白饭的味道，是百味之基。饭之甘，更在百味之上。其甘，是符合自然之道的味，是粮食本身的甘，其美，是得自日月山川的美。>>>new>>>符合自然的它你抚平的它你抚平的它你的有的，何为??吃吃，几口，。饭，它?你的总是吃三口：：：：：：：：：：：：：：：：。\n",
      "Epoch 34 Batch 0 Loss 0.3435\n",
      "Epoch 34 finished, Loss: 1.1298, Time cost: 8.349118709564209 seconds\n",
      "\n",
      "在宴席上，饭，一般被称为主食;然而，它总在宴席的尾声，才姗姗上来。那时，美酒肴馔已占据胃肠;饭，往往成了陪衬。大家，也总象征性地来几口，压个轴。>>>new>>>，思在在在饭，是日月抚平的它你本身的，才姗姗上来。的;饭，它抚平你的你的它??的有的菜，为主，何为有的传承的。\n",
      "Epoch 35 Batch 0 Loss 0.3287\n",
      "Epoch 35 finished, Loss: 1.0690, Time cost: 8.354602336883545 seconds\n",
      "\n",
      "旧时好多有传承的人家，吃饭有个规矩：一桌子琳琅佳肴前，先吃三口白饭。长辈一代代教诲：第一口必须先吃饭，而绝不能没吃饭就夹菜。>>>new>>>。多多多五味杂杂，它抚平你的时分。\n",
      "Epoch 36 Batch 0 Loss 0.3107\n",
      "Epoch 36 finished, Loss: 1.0158, Time cost: 9.114558219909668 seconds\n",
      "\n",
      "我想，三口白饭，是提醒你：食之本，在饭;饭之味，为源。饭味为正味，正味恬淡素朴。一碗白饭的味道，是百味之基。饭之甘，更在百味之上。其甘，是符合自然之道的味，是粮食本身的甘，其美，是得自日月山川的美。>>>new>>>符合自然的自然的它五味杂，它抚平本身的，也饭，它抚平你的初恋的有的。三口，饭，被称为主食;饭，它抚平你的自然的它抚平你的它抚平本身的，。\n",
      "Epoch 37 Batch 0 Loss 0.2915\n",
      "Epoch 37 finished, Loss: 0.9607, Time cost: 8.824098825454712 seconds\n",
      "\n",
      "这个规矩有来历。明朝的一部养生专著《遵生八笺》中说到，一位僧人，吃饭总是先淡吃三口：“第一，以知饭之正味。人食多以五味杂之，未有知正味者，若淡食，则本自甘美，初不假外味也。第二，思衣食之从来。第三，思农夫之艰苦。”>>>new>>>自然的自然之道的味，是百味在在饭，是自日月的它抚平心头的味，是粮食。的心头的它你心头的的成称为主食;饭，它你你的时分。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 Batch 0 Loss 0.2885\n",
      "Epoch 38 finished, Loss: 0.9284, Time cost: 8.411935806274414 seconds\n",
      "\n",
      "它滋养人身，也颐养人心。 >>>new>>>，在在宴席的尾声，才姗姗上来。，称为饭，先三口，在在在饭，则自日月人心。\n",
      "Epoch 39 Batch 0 Loss 0.2655\n",
      "Epoch 39 finished, Loss: 0.8858, Time cost: 8.378061532974243 seconds\n",
      "\n",
      "饭，的确是最养人的东西。一方水土养一方人，大江南北饭不同：南方人吃白米饭，类似于北方人吃馒头面条，也类似于西北高原吃特色面食……一餐一饭，化成人的血肉，供养着人的生命。漂泊时，它抚平你心头的创伤;安逸时，它像你初恋时分的纯洁思想。>>>new>>>胃肠;饭，它粮食本身的甘，也在在在宴席的也。也在在在饭，是自日月的它抚平本身的甘，也总在饭，是自日月的。\n",
      "Epoch 40 Batch 0 Loss 0.2576\n",
      "Epoch 40 finished, Loss: 0.7999, Time cost: 8.376927852630615 seconds\n",
      "\n",
      "这个规矩有来历。明朝的一部养生专著《遵生八笺》中说到，一位僧人，吃饭总是先淡吃三口：“第一，以知饭之正味。人食多以五味杂之，未有知正味者，若淡食，则本自甘美，初不假外味也。第二，思衣食之从来。第三，思农夫之艰苦。”>>>new>>>自然之从来。”;在在在饭，本自甘美，是符合自然之道的味，是粮食本身的甘，其美，。\n",
      "Epoch 41 Batch 0 Loss 0.2270\n",
      "Epoch 41 finished, Loss: 0.7384, Time cost: 8.435436248779297 seconds\n",
      "\n",
      "我想，三口白饭，是提醒你：食之本，在饭;饭之味，为源。饭味为正味，正味恬淡素朴。一碗白饭的味道，是百味之基。饭之甘，更在百味之上。其甘，是符合自然之道的味，是粮食本身的甘，其美，是得自日月山川的美。>>>new>>>。多多以五味杂之，它粮食本身的甘，饭也。\n",
      "Epoch 42 Batch 0 Loss 0.2125\n",
      "Epoch 42 finished, Loss: 0.6900, Time cost: 8.43098521232605 seconds\n",
      "\n",
      "我想，三口白饭，是提醒你：食之本，在饭;饭之味，为源。饭味为正味，正味恬淡素朴。一碗白饭的味道，是百味之基。饭之甘，更在百味之上。其甘，是符合自然之道的味，是粮食本身的甘，其美，是得自日月山川的美。>>>new>>>。多多五味杂杂，它总在宴席的尾声，也总象征性地来几口，压个轴。\n",
      "Epoch 43 Batch 0 Loss 0.1968\n",
      "Epoch 43 finished, Loss: 0.6449, Time cost: 9.487030506134033 seconds\n",
      "\n",
      "它滋养人身，也颐养人心。 >>>new>>>：：食之本，饭，总象征性地来几口，压个，本自甘美，初不假也。百味在在宴席的尾声，才姗姗上来。\n",
      "Epoch 44 Batch 0 Loss 0.1824\n",
      "Epoch 44 finished, Loss: 0.6090, Time cost: 8.663391828536987 seconds\n",
      "\n",
      "这个规矩有来历。明朝的一部养生专著《遵生八笺》中说到，一位僧人，吃饭总是先淡吃三口：“第一，以知饭之正味。人食多以五味杂之，未有知正味者，若淡食，则本自甘美，初不假外味也。第二，思衣食之从来。第三，思农夫之艰苦。”>>>new>>>之，未有知正味者，若淡食，则本自甘美，初不假外味也。第二，思衣食之从来。\n",
      "Epoch 45 Batch 0 Loss 0.1796\n",
      "Epoch 45 finished, Loss: 0.5865, Time cost: 8.41748046875 seconds\n",
      "\n",
      "这个规矩有来历。明朝的一部养生专著《遵生八笺》中说到，一位僧人，吃饭总是先淡吃三口：“第一，以知饭之正味。人食多以五味杂之，未有知正味者，若淡食，则本自甘美，初不假外味也。第二，思衣食之从来。第三，思农夫之艰苦。”>>>new>>>自然之从来。第三，思思农夫之艰苦。\n",
      "Epoch 46 Batch 0 Loss 0.1582\n",
      "Epoch 46 finished, Loss: 0.5465, Time cost: 8.578631162643433 seconds\n",
      "\n",
      "一餐饭中，饭与菜，何为主，何为次?>>>new>>>。多多以五味杂之，未有知正味者，若淡食，则本自甘美，初不假外味也。\n",
      "Epoch 47 Batch 0 Loss 0.1449\n",
      "Epoch 47 finished, Loss: 0.5098, Time cost: 8.429327487945557 seconds\n",
      "\n",
      "它滋养人身，也颐养人心。 >>>new>>>白饭。正味，思农夫之艰苦。\n",
      "Epoch 48 Batch 0 Loss 0.1354\n",
      "Epoch 48 finished, Loss: 0.4673, Time cost: 8.38213849067688 seconds\n",
      "\n",
      "一餐饭中，饭与菜，何为主，何为次?>>>new>>>。一代代教诲第一口必须先吃饭，而绝不能没吃饭就夹菜。\n",
      "Epoch 49 Batch 0 Loss 0.1268\n",
      "Epoch 49 finished, Loss: 0.4479, Time cost: 8.347826719284058 seconds\n",
      "\n",
      "旧时好多有传承的人家，吃饭有个规矩：一桌子琳琅佳肴前，先吃三口白饭。长辈一代代教诲：第一口必须先吃饭，而绝不能没吃饭就夹菜。>>>new>>>白饭。正味，思农夫之艰苦。\n",
      "Epoch 50 Batch 0 Loss 0.1367\n",
      "Epoch 50 finished, Loss: 0.4323, Time cost: 9.60459017753601 seconds\n",
      "\n",
      "我想，三口白饭，是提醒你：食之本，在饭;饭之味，为源。饭味为正味，正味恬淡素朴。一碗白饭的味道，是百味之基。饭之甘，更在百味之上。其甘，是符合自然之道的味，是粮食本身的甘，其美，是得自日月山川的美。>>>new>>>自然之道的味，是粮食本身的甘，其美，是得自日月山川的美。恬淡。\n"
     ]
    }
   ],
   "source": [
    "writer.train(epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved to: language_model_short\n"
     ]
    }
   ],
   "source": [
    "writer.save_model(\"language_model_short\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'旧时好多有传承的人家，吃饭有个规矩：一桌子琳琅佳肴前，先吃三口白饭。长辈一代代教诲：第一口必须先吃饭，而绝不能没吃饭就夹菜。>>>new>>>。一碗白饭的味道，是百味在宴席的尾声，才姗姗上来。那时，美酒已占据胃肠;饭，往往成了陪衬。大家，也总象征性地来几口，压个轴。其甘，是符合自然之道的味，是粮食本身的甘，其美，是得自日月山川的美。'"
      ]
     },
     "execution_count": 673,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writer.write(content[3][:50], \"。\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'柴米油盐酱醋茶>>>new>>>一代代教诲：第一口必须先吃饭，而绝不能没吃饭就夹菜。第三，思农夫之艰苦。”一碗是之自然之道的味，是粮食本身的甘，其美，是得自日月山川的美。恬淡。思思衣食之从来。'"
      ]
     },
     "execution_count": 681,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writer.write(\"柴米油盐酱醋茶\", \"。\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试了长度不同的小说和文章，基本结果都是大同小异，从一开始的持续输出符号，到重复高频词语的胡言乱语，再到稍微有些意义的句子…需要的训练轮数和词表长度关系比较大，同时短一些的文章也需要训练更多的epoch，另外，输入相同时输出内容总是完全相同的，感觉不太科学，不知道需不需要加入一定随机性，而且文章过短时会发生模型背诵全文的情况…大致上感觉模型应该是可用的，只是长文章需要更多epoch的训练，时间关系就不再多做尝试了"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
